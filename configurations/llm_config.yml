llm_config:
  model_path: "models/Llama-2-7B-Chat-GGUF/llama-2-7b-chat.Q8_0.gguf"
  model_type: "Llama"
  n_gpu_layers: 30
  n_ctx: 840
  n_batch: 840
  n_threads: 8
  temperature: 0.7
  top_p: 0.5
  top_k: 40
  repeat_penalty: 1.17647
  last_n_tokens: 256
  max_tokens: 256
  f16_kv: True
  verbose: True
